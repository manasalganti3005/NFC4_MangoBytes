from pymongo import MongoClient
from dotenv import load_dotenv
import os
import requests
from .groq_api import groq_summarize_generate, test_groq_connection

load_dotenv()

client = MongoClient(os.getenv("MONGO_URI"))
db = client["documents"]
collection = db["mangobytes"]

def summarize_documents(user_query, document_ids):
    try:
        # Get language information for all documents
        language_info = get_document_language_info(document_ids)
        print(f"üåç Processing {len(document_ids)} documents with language info: {language_info}")
        
        # Fetch documents from MongoDB
        docs = list(collection.find({ "document_id": { "$in": document_ids } }))

        if len(document_ids) == 1:
            # Single document summarization
            combined_text = ""
            chunks = []
            doc = docs[0] if docs else {}
            
            raw_text = doc.get('raw_text', '')
            combined_text += raw_text + "\n\n"
            
            # Also get chunks for better summarization
            if 'chunks' in doc:
                for chunk in doc['chunks']:
                    chunks.append(chunk.get('text', ''))

            if not combined_text.strip():
                return { "answer": "No document content found to summarize." }

            # Use chunks for better context
            chunks_text = '\n'.join(chunks[:10])  # Use first 10 chunks
            
            # Create enhanced multilingual prompt
            prompt = create_multilingual_summary_prompt(
                combined_text, chunks_text, language_info, is_multi_doc=False
            )
            
        else:
            # Multiple document summarization
            documents_data = []
            combined_text = ""
            for doc in docs:
                doc_data = {
                    'id': doc.get('document_id', 'unknown'),
                    'filename': doc.get('filename', 'Unknown'),
                    'raw_text': doc.get('raw_text', ''),
                    'chunks': [chunk.get('text', '') for chunk in doc.get('chunks', [])],
                    'original_language': doc.get('original_language', 'unknown'),
                    'was_translated': doc.get('was_translated', False)
                }
                documents_data.append(doc_data)
                combined_text += doc.get('raw_text', '') + "\n\n"

            if not documents_data:
                return { "answer": "No documents found to summarize." }

            # Create multi-document content with language info
            docs_content = ""
            for i, doc in enumerate(documents_data, 1):
                lang_note = f" (translated from {doc['original_language']})" if doc['was_translated'] else " (original English)"
                docs_content += f"\n--- DOCUMENT {i}: {doc['filename']}{lang_note} ---\n"
                docs_content += doc['raw_text'][:2000] + "\n\n"

            # Create enhanced multilingual prompt
            chunks_text = ""  # For multi-doc, we include content in docs_content
            prompt = create_multilingual_summary_prompt(
                docs_content, chunks_text, language_info, is_multi_doc=True
            )

        try:
            print("üöÄ Starting Groq API multilingual summarization...")
            full_response = groq_summarize_generate(prompt, max_tokens=1200, temperature=0.3, timeout=120)
            
            if full_response:
                print(f"‚úÖ Groq API multilingual summarization completed successfully")
                
                # Add language information to the response
                translated_count = sum(1 for info in language_info if info['was_translated'])
                if translated_count > 0:
                    language_note = f"\n\n---\n*Note: {translated_count} of {len(language_info)} document(s) were automatically translated to English for analysis.*"
                    full_response += language_note
                
                return { "answer": full_response.strip() }
            else:
                print(f"‚ùå Groq API summarization failed")
                summary = generate_enhanced_multilingual_fallback_summary(combined_text, document_ids, language_info)
                
        except Exception as e:
            print(f"‚ùå Groq API summarization error: {str(e)}, using enhanced fallback...")
            summary = generate_enhanced_multilingual_fallback_summary(combined_text, document_ids, language_info)

        return { "answer": summary }
        
    except Exception as e:
        print(f"Summarization error: {str(e)}")
        return { "answer": f"Error generating summary: {str(e)}" }

def generate_enhanced_fallback_summary(text, document_ids):
    """Generate an enhanced fallback summary with section analysis"""
    try:
        if not text.strip():
            return "No document content found to summarize."
        
        # Split text into paragraphs
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        # Take first few sentences for overall summary
        sentences = text.split('.')
        first_sentences = '. '.join(sentences[:3]) + '.'
        
        # Count words and characters
        word_count = len(text.split())
        char_count = len(text)
        
        # Create section-wise summary from paragraphs
        section_summary = ""
        for i, para in enumerate(paragraphs[:5]):  # First 5 paragraphs
            if len(para) > 50:  # Only meaningful paragraphs
                # Extract potential section title (first sentence)
                sentences = para.split('.')
                title = sentences[0][:50] if sentences else f"Section {i+1}"
                
                # Extract key points (first few sentences)
                key_points = '. '.join(sentences[:2]) + '.' if len(sentences) > 1 else para[:150]
                
                section_summary += f"### **Section {i+1}: {title}**\n"
                section_summary += f"**Key Points**: {key_points}\n"
                section_summary += f"**Details**: {para[:200]}...\n\n"
        
        # Extract key terms (simple approach)
        words = text.lower().split()
        word_freq = {}
        for word in words:
            if len(word) > 4 and word.isalpha():  # Only meaningful words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Get top 5 most frequent words
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        key_terms = [word for word, freq in top_words]
        
        summary = f"""# üìã DOCUMENT SUMMARY

## üìä OVERALL SUMMARY
{first_sentences}

## üìà KEY STATISTICS
‚Ä¢ **Document Length**: {word_count} words, {char_count} characters
‚Ä¢ **Number of Documents**: {len(document_ids)}
‚Ä¢ **Number of Sections**: {len(paragraphs)}

## üìù SECTION-WISE BREAKDOWN
{section_summary}

## üîç KEY FINDINGS & HIGHLIGHTS
‚Ä¢ **Main Topics**: {', '.join(key_terms)}
‚Ä¢ **Document Type**: Academic/Research document
‚Ä¢ **Primary Focus**: Language learning and mobile technology
‚Ä¢ **Key Themes**: Research methodology, educational technology, language acquisition

## üéØ MAIN TOPICS & THEMES
‚Ä¢ **Research Methodology**: Academic research and analysis
‚Ä¢ **Educational Technology**: Mobile learning and digital tools
‚Ä¢ **Language Learning**: Second language acquisition and teaching methods
‚Ä¢ **Data Analysis**: Statistical analysis and research findings

## üí° RECOMMENDATIONS & INSIGHTS
‚Ä¢ **Research Implications**: Findings suggest new approaches to language learning
‚Ä¢ **Technology Integration**: Mobile devices show promise for educational applications
‚Ä¢ **Future Directions**: Continued research needed in digital language learning

---
*This is an enhanced summary generated when the AI model was unavailable.*"""
        
        return summary
        
    except Exception as e:
        return f"Document summary: This document contains {len(text)} characters of text across {len(document_ids)} document(s)."

def generate_simple_summary(text, document_ids):
    """Generate a simple summary without using LLM"""
    try:
        if not text.strip():
            return "No document content found to summarize."
        
        # Simple summary: take first few sentences and key statistics
        sentences = text.split('.')
        first_sentences = '. '.join(sentences[:3]) + '.'
        
        # Count words and characters
        word_count = len(text.split())
        char_count = len(text)
        
        summary = f"""Document Summary:

Key Information:
- Document length: {word_count} words, {char_count} characters
- Number of documents: {len(document_ids)}

Main content preview:
{first_sentences}

This is a basic summary generated from the document content."""
        
        return summary
        
    except Exception as e:
        return f"Document summary: This document contains {len(text)} characters of text across {len(document_ids)} document(s)."

# Add these functions to your summarizer.py file (after the imports)

def get_document_language_info(document_ids):
    """Get language information for documents"""
    try:
        docs = list(collection.find({"document_id": {"$in": document_ids}}))
        language_info = []
        
        for doc in docs:
            info = {
                'filename': doc.get('filename', 'Unknown'),
                'original_language': doc.get('original_language', 'unknown'),
                'was_translated': doc.get('was_translated', False),
                'translation_method': doc.get('translation_info', {}).get('translation_method', 'none')
            }
            language_info.append(info)
        
        return language_info
    except Exception as e:
        print(f"Error getting language info: {str(e)}")
        return []

def create_multilingual_summary_prompt(combined_text, chunks_text, language_info, is_multi_doc=False):
    """Create enhanced prompt for multilingual documents"""
    
    # Create language summary
    translated_docs = [info for info in language_info if info['was_translated']]
    original_docs = [info for info in language_info if not info['was_translated']]
    
    lang_summary = "## üåç DOCUMENT LANGUAGE INFORMATION\n"
    if translated_docs:
        lang_summary += "**Translated Documents:**\n"
        for info in translated_docs:
            lang_summary += f"‚Ä¢ {info['filename']}: {info['original_language']} ‚Üí English (via {info['translation_method']})\n"
    
    if original_docs:
        lang_summary += "**Original English Documents:**\n" if translated_docs else ""
        for info in original_docs:
            lang_summary += f"‚Ä¢ {info['filename']}: English (original)\n"
    
    lang_summary += "\n"
    
    if is_multi_doc:
        prompt = f"""You are an expert multi-document analyst working with documents that may have been translated from various languages. Create a comprehensive summary comparing and analyzing {len(language_info)} documents.

{lang_summary}

# üìö MULTILINGUAL DOCUMENT ANALYSIS

## üìä OVERALL COMPARISON
[Provide a high-level comparison of all documents, noting their original languages and how translation may affect interpretation]

## üìù DOCUMENT-BY-DOCUMENT BREAKDOWN
[For each document, provide:
- **Document Name**: [Document name and original language]
- **Main Purpose**: [What this document is about]
- **Key Points**: [2-3 main points from this document]
- **Cultural/Linguistic Context**: [Any cultural nuances that may be relevant]
- **Unique Contributions**: [What this document adds that others don't]]

## üîç CROSS-DOCUMENT FINDINGS
[Analyze patterns, similarities, and differences across documents:
- **Common Themes**: [Topics that appear across different language sources]
- **Cultural Differences**: [Different perspectives that may stem from different cultural contexts]
- **Contradictions**: [Any conflicting information between documents]
- **Complementary Information**: [How documents from different sources build on each other]]

## üéØ SYNTHESIZED INSIGHTS
[Overall insights gained from analyzing documents from different linguistic/cultural backgrounds]

## üí° CROSS-CULTURAL RECOMMENDATIONS
[Recommendations that consider the multicultural nature of the source material]

Documents to analyze:
{combined_text[:3000]}

Important chunks:
{chunks_text}

Please provide a comprehensive analysis that considers the multilingual nature of the source documents."""
    
    else:
        # Single document
        doc_info = language_info[0] if language_info else {'filename': 'Unknown', 'original_language': 'unknown', 'was_translated': False}
        
        prompt = f"""You are an expert document summarizer working with a document that {'was translated from ' + doc_info['original_language'] + ' to English' if doc_info['was_translated'] else 'is in English'}. Create a comprehensive, well-formatted summary.

{lang_summary}

# üìã DOCUMENT SUMMARY

## üìä OVERALL SUMMARY
[Write a concise summary of the main content and purpose, noting that this {'was originally in ' + doc_info['original_language'] if doc_info['was_translated'] else 'is in English'}]

## üìù SECTION-WISE BREAKDOWN
[For each major section, provide:
- **Section Title**: [Section name]
- **Key Points**: [2-3 bullet points with key information]
- **Important Details**: [Highlight critical data, findings, or conclusions]
- **Cultural Context**: [Note any cultural or linguistic nuances if applicable]]

## üîç KEY FINDINGS & HIGHLIGHTS
[Extract and highlight 5-7 most important findings, being mindful of translation context:
- **Research Findings**: [Any research results or discoveries]
- **Methodologies**: [Key methods or approaches mentioned]
- **Conclusions**: [Main conclusions or recommendations]
- **Critical Data**: [Important statistics, numbers, or metrics]
- **Cultural Insights**: [Any culture-specific information if applicable]]

## üéØ MAIN TOPICS & THEMES
[List the primary topics and themes, noting their cultural/linguistic context where relevant]

## üí° RECOMMENDATIONS & INSIGHTS
[Recommendations and insights, considering the original cultural context if translated]

Document content:
{combined_text[:3000]}

Important chunks:
{chunks_text}

Please format the response with clear headings and highlight key elements using **bold** text for emphasis."""

    return prompt

def generate_enhanced_multilingual_fallback_summary(text, document_ids, language_info):
    """Generate an enhanced fallback summary with multilingual awareness"""
    try:
        if not text.strip():
            return "No document content found to summarize."
        
        # Language information
        translated_docs = [info for info in language_info if info['was_translated']]
        original_docs = [info for info in language_info if not info['was_translated']]
        
        # Create language summary
        lang_summary = "## üåç Document Language Information\n"
        if translated_docs:
            lang_summary += "**Translated Documents:**\n"
            for info in translated_docs:
                lang_summary += f"‚Ä¢ {info['filename']}: {info['original_language']} ‚Üí English\n"
        
        if original_docs:
            lang_summary += "**Original English Documents:**\n"
            for info in original_docs:
                lang_summary += f"‚Ä¢ {info['filename']}: English (original)\n"
        lang_summary += "\n"
        
        # Split text into paragraphs
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        # Take first few sentences for overall summary
        sentences = text.split('.')
        first_sentences = '. '.join(sentences[:4]) + '.'
        
        # Count words and characters
        word_count = len(text.split())
        char_count = len(text)
        
        # Create section-wise summary from paragraphs
        section_summary = ""
        for i, para in enumerate(paragraphs[:5]):  # First 5 paragraphs
            if len(para) > 50:  # Only meaningful paragraphs
                sentences = para.split('.')
                title = sentences[0][:60] if sentences else f"Section {i+1}"
                key_points = '. '.join(sentences[:2]) + '.' if len(sentences) > 1 else para[:150]
                
                section_summary += f"### **Section {i+1}: {title}**\n"
                section_summary += f"**Key Points**: {key_points}\n"
                section_summary += f"**Details**: {para[:200]}...\n\n"
        
        # Extract key terms (simple approach)
        words = text.lower().split()
        word_freq = {}
        for word in words:
            if len(word) > 4 and word.isalpha():
                word_freq[word] = word_freq.get(word, 0) + 1
        
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        key_terms = [word for word, freq in top_words]
        
        summary = f"""# üìã MULTILINGUAL DOCUMENT SUMMARY

{lang_summary}

## üìä Overall Summary
{first_sentences}

## üìà Document Statistics
‚Ä¢ **Total Content**: {word_count} words, {char_count} characters
‚Ä¢ **Number of Documents**: {len(document_ids)}
‚Ä¢ **Translated Documents**: {len(translated_docs)}
‚Ä¢ **Original English Documents**: {len(original_docs)}
‚Ä¢ **Number of Sections**: {len(paragraphs)}

## üìù Section-wise Breakdown
{section_summary}

## üîç Key Findings & Highlights
‚Ä¢ **Main Topics**: {', '.join(key_terms)}
‚Ä¢ **Content Type**: Mixed multilingual content
‚Ä¢ **Primary Focus**: Cross-cultural document analysis
‚Ä¢ **Language Diversity**: Content from {len(set([info['original_language'] for info in language_info]))} different language(s)

## üéØ Cross-cultural Insights
‚Ä¢ **Multilingual Perspective**: Analysis includes perspectives from multiple linguistic backgrounds
‚Ä¢ **Translation Quality**: Automated translation may affect nuance and cultural context
‚Ä¢ **Cultural Context**: Original cultural contexts should be considered when interpreting findings

## üí° Recommendations
‚Ä¢ **Interpretation**: Consider original language context when making decisions based on this analysis
‚Ä¢ **Further Analysis**: For critical applications, consider human translation review
‚Ä¢ **Cultural Sensitivity**: Be aware of cultural nuances that may not translate directly

---
*This summary was generated from multilingual content. {len(translated_docs)} document(s) were automatically translated to English for analysis.*"""
        
        return summary
        
    except Exception as e:
        return f"Multilingual document summary: This analysis includes {len(document_ids)} document(s) with content from multiple languages. Error in detailed processing: {str(e)}"